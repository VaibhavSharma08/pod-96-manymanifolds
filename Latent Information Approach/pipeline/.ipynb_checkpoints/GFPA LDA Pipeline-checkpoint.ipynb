{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluralistic Approach to Decoding Motor Activity from Different Regions in the Rodent Brain\n",
    "##### Project Contributors: Narotam Singh, Vaibhav, Rishika Mohanta, Prakriti Nayak\n",
    "\n",
    "##### Done as part of [Neuromatch Academy](https://github.com/NeuromatchAcademy/course-content) July 13-31 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Information Approach Pipeline\n",
    "\n",
    "The original data is from Steinmetz, Nicholas A., et al. \"Distributed coding of choice, action and engagement across the mouse brain.\" Nature 576.7786 (2019): 266-273. It was then further cleaned to consider only recordings from motor related areas with more than 50 neurons from atleast 2 mice. We only considered the the open loop condition ie. data between stimulus onset and go cue to avoid representations of moving stimulus from appearing in the neural data we are analysing. \n",
    "\n",
    "We implemented a pipeline to compute latent dimensions of the neural spike data from 50 randomly sampled neurons from 100 randomly sampled trials using the Gaussian Process Factor Analysis (Yu, 2009) implemention in the ELEctroPHysiology ANalysis Toolbox (``Elephant`` package in ``python 3.6``). This was followed by a Ridge Regression to reconstruct motor output (``wheel``) and Linear Discriminant Analysis (LDA) to classify left motor output, right motor output and no motor output.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import neo as neo\n",
    "import quantities as q\n",
    "import elephant as elephant\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = np.load('../cleaned_dataset/train.npz',allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of best number of latent dimensions\n",
    "\n",
    "Due to time limitations, we had to determine a reasonably good estimate of the number of latent dimensions we need for which we used a Cross Validation Approach to maximize the log-likelihood on 5 randomly sampled sessions and areas. A better alternative would be to determine and analyse the best latent representations for all samples but it is computationally expensive. Thus we took the random sampling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to estimate best number of latent dimension\n",
    "\n",
    "def evaluate_best_ld(dat,brain_region,n_neurons=50,n_trials=100,dim_res=2,GPFAcv=5,limits=(-0.1,0.1),LDAcv=10):\n",
    "    print(f\"Brain Region: {brain_region}, Number of Neurons: {n_neurons}, Number of Trials: {n_trials}\")   \n",
    "    ### Choose brain area\n",
    "    neurons = dat['brain_area'] == brain_region\n",
    "    ### Create Neo Dataset\n",
    "    y = np.zeros((1,))\n",
    "    data = []\n",
    "    # Choose Neurons\n",
    "    n_set = np.random.choice(np.arange(sum(neurons)),size=n_neurons,replace=False)\n",
    "    # Choose Trials\n",
    "    t_set = np.random.choice(np.arange(dat['spks'].shape[1]),size=n_trials,replace=False)\n",
    "    for trials in t_set:\n",
    "        start = int(dat[\"stim_onset\"]/dat[\"bin_size\"])\n",
    "        end = int((dat[\"stim_onset\"]+dat[\"gocue\"][trials])/dat[\"bin_size\"])\n",
    "        spikes = dat['spks'][neurons,:,:][n_set,:,:][:,trials,:][:,start:end]\n",
    "        trial = []\n",
    "        for i in range(spikes.shape[0]):\n",
    "            trial.append(neo.SpikeTrain(np.argwhere(spikes[i,:]==1).flatten()*10.0*q.ms,t_stop=(end-start)*10.0*q.ms))\n",
    "        y = np.concatenate((y,dat['wheel'][0][trials,start:end]))\n",
    "        data.append(trial)\n",
    "    print(\"GPFA Training Data Generated...\")\n",
    "    y=y[1:]\n",
    "    ### CV to evaluate number of latent dimension\n",
    "    print(\"Starting CV Latent Space Evaluation...\")    \n",
    "    x_dims = np.linspace(1,n_neurons,1+int(n_neurons/dim_res))\n",
    "    log_likelihoods = []\n",
    "    iterator = 0\n",
    "    for x_dim in x_dims:\n",
    "        print(f\"Evaluating D = {x_dim} ({iterator}/{len(x_dims)})\")\n",
    "        iterator +=1\n",
    "        # estimate the log-likelihood for the given dimensionality \n",
    "        gpfa_cv = elephant.gpfa.GPFA(bin_size=10*q.ms,x_dim=int(x_dim))\n",
    "        try:\n",
    "            cv_log_likelihoods = cross_val_score(gpfa_cv, data, cv=GPFAcv, n_jobs=GPFAcv, verbose=True)\n",
    "        except:\n",
    "            print(f\"Didnt work for {x_dim}\")\n",
    "        log_likelihoods.append((np.mean(cv_log_likelihoods),cv_log_likelihoods))\n",
    "    output = {\n",
    "        'neurons':n_set,\n",
    "        'trials':t_set,\n",
    "        'data':data,\n",
    "        'x_dims':x_dims,\n",
    "        'cv_ll_scores':log_likelihoods\n",
    "    }\n",
    "    print(\"Completed.\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly sample 5 (Session, Brain Area) pairs and compute Maximum Log-Likelihood Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Sample\n",
    "session_region = [] \n",
    "for n,i in enumerate(alldat):\n",
    "    for j in np.unique(i['brain_area']):\n",
    "        session_region.append((n,j))\n",
    "r_sample = list(np.int32(np.random.choice(len(session_region),size=5,replace=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Best latent dimensions\n",
    "for i in r_sample:\n",
    "    a = evaluate_best_ld(alldat[session_region[i][0]],session_region[i][1],dim_res=2)\n",
    "    np.save(f\"../results/GFPA-maxLL-data/{session_region[i][1]}_{session_region[i][0]}\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "for n,i in enumerate(r_sample):\n",
    "    plt.subplot(5,1,n+1)\n",
    "    a = np.load(f\"{session_region[i][1]}_{session_region[i][0]}.npy\",allow_pickle=True)\n",
    "    a = a.item()\n",
    "    plt.plot(a['x_dims'],[temp[0] for temp in a['cv_ll_scores']])\n",
    "    plt.title(f\"../results/GFPA-maxLL-data/{session_region[i][1]}_{session_region[i][0]}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 latent dimensions seems to be a reasonable good estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPFA+LDA Computation\n",
    "\n",
    "With an 20 dimensional latent space, we compute the GPFA followed by LDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_data(dat,brain_region,n_neurons=50,n_trials=100,GPFAcv=5,limits=(-0.1,0.1),LDAcv=5):\n",
    "    print(\"### Welcome to Latent Space Seperation Analysis ###\")\n",
    "    print(f\"Brain Region: {brain_region}, Number of Neurons: {n_neurons}, Number of Trials: {n_trials}\\n\")   \n",
    "    ### Choose brain area\n",
    "    neurons = dat['brain_area'] == brain_region\n",
    "    ### Create Neo Dataset\n",
    "    y = np.zeros((1,))\n",
    "    data = []\n",
    "    # Choose Neurons\n",
    "    n_set = np.random.choice(np.arange(sum(neurons)),size=n_neurons,replace=False)\n",
    "    # Choose Trials\n",
    "    t_set = np.random.choice(np.arange(dat['spks'].shape[1]),size=n_trials,replace=False)\n",
    "    for trials in t_set:\n",
    "        start = int(dat[\"stim_onset\"]/dat[\"bin_size\"])\n",
    "        end = int((dat[\"stim_onset\"]+dat[\"gocue\"][trials])/dat[\"bin_size\"])\n",
    "        spikes = dat['spks'][neurons,:,:][n_set,:,:][:,trials,:][:,start:end]\n",
    "        trial = []\n",
    "        for i in range(spikes.shape[0]):\n",
    "            trial.append(neo.SpikeTrain(np.argwhere(spikes[i,:]==1).flatten()*10.0*q.ms,t_stop=(end-start)*10.0*q.ms))\n",
    "        y = np.concatenate((y,dat['wheel'][0][trials,start:end]))\n",
    "        data.append(trial)\n",
    "    print(\"GPFA Training Data Generated...\")\n",
    "    y=y[1:]\n",
    "    ### Evaluate full space GPFA\n",
    "    gpfa = elephant.gpfa.GPFA(bin_size=10*q.ms, x_dim=20)\n",
    "    start_time = time.time()\n",
    "    trajectories = gpfa.fit_transform(data)\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nFitting Complete {(end_time-start_time)/60:0.1f} mins Elapsed.\\nLatent Space Transfomed...\")\n",
    "    ### Prepare Classifier Data\n",
    "    X_data = np.concatenate(trajectories,axis=1).T\n",
    "    y_data = np.int32(y>limits[1])-np.int32(y<limits[0])\n",
    "    print(\"Discriminant Analysis Starting...\")\n",
    "    print(f\"Classification Limits: ({limits[0]} , {limits[1]}) , K-Fold CV K = {LDAcv}\")\n",
    "    LDAmodel = LinearDiscriminantAnalysis()\n",
    "    LDAscores = cross_validate(LDAmodel, X=X_data,y=y_data, cv=LDAcv, n_jobs=LDAcv, return_train_score=True, verbose=True,scoring=['balanced_accuracy','f1_weighted'])\n",
    "    lds = LDAmodel.fit_transform(X_data,y_data)\n",
    "    print(\"Linear Discriminant Analysis Completed ... Outputing\")\n",
    "    output = {\n",
    "        'neurons':n_set,\n",
    "        'trials':t_set,\n",
    "        'data':data,\n",
    "        'gpfa_model':gpfa,\n",
    "        'gpfa_traj':trajectories,\n",
    "        'X_data':X_data,\n",
    "        'y_clas':y_data,\n",
    "        'y_reg':y,\n",
    "        'lda_model':LDAmodel,\n",
    "        'cv_lda_scores':LDAscores,\n",
    "        'lda_traj':lds\n",
    "    }\n",
    "    print(\"Completed.\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPFA analysis for all (Session, Brain Area) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_region = [] \n",
    "for n,i in enumerate(alldat):\n",
    "    for j in np.unique(i['brain_area']):\n",
    "        session_region.append((n,j))\n",
    "\n",
    "overall_information = []\n",
    "\n",
    "for i in tqdm(session_region):\n",
    "    for j in range(5):\n",
    "        output = evaluate_data(alldat[i[0]],i[1])\n",
    "        np.save(f'../results/GPFA-LDA-data/{i[1]}_{i[0]}_{j}.npy',np.array(output))\n",
    "        overall_information.append([\n",
    "            i[0],\n",
    "            i[1],\n",
    "            j,\n",
    "            output['neurons'],\n",
    "            output['trials'],\n",
    "            np.mean(output['cv_lda_scores']['test_balanced_accuracy']),\n",
    "            np.mean(output['cv_lda_scores']['test_f1_weighted']),\n",
    "            np.mean(output['cv_lda_scores']['train_balanced_accuracy']),\n",
    "            np.mean(output['cv_lda_scores']['train_f1_weighted'])\n",
    "        ])\n",
    "\n",
    "df = pd.DataFrame(data=overall_information,columns=['session_no','brain_area','replicate_id',\n",
    "                                               'neuron_idx','trial_idx',\n",
    "                                               'mean_test_accuracy','mean_test_f1',\n",
    "                                               'mean_train_accuracy','mean_train_f1'])\n",
    "df.to_pickle('../results/GPFA-LDA-result-summary.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
